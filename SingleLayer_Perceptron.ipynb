{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "First let's create a few data points to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "Y: [[0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_input = np.array([[0,0,1,1],[0,1,0,1]]) # input values for our model\n",
    "Y_output = np.array([[0,1,1,1]]) # 'actuals'\n",
    "\n",
    "print(f\"X: {X_input}\\n\")\n",
    "print(f\"Y: {Y_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will define the input/output size and print out our Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 2\n",
      "Output size: 1\n",
      "learning_rate: 0.1\n",
      "epochs: 20\n"
     ]
    }
   ],
   "source": [
    "# define input/output size\n",
    "input_size = X_input.shape[0]\n",
    "output_size = Y_output.shape[0]\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = .1 # the amount we will multiply our weights and bias by (impacts our rate of adjustments)\n",
    "epochs = 20 # number of times we will iterate through our model \n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will create our starting weight and bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: \n",
      "[[0.00534368 0.00788244]]\n",
      "\n",
      "Bias:\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "Weight = np.random.randn(output_size, input_size) * .01 # small random value\n",
    "Bias = np.zeros((output_size, 1)) # all zeros\n",
    "\n",
    "print(f\"Weight: \\n{Weight}\\n\")\n",
    "print(f\"Bias:\\n{Bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally with the parameters/hyperparams set we will run through our Single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.5        0.5019706  0.50133592 0.50330648]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.6898489502434526\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.00534368 0.00788244]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.2488394  -0.24868073]]\n",
      "New Weight: [[0.03022762 0.03275051]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.24834675]]\n",
      "New Bias: [[0.02483467]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.50620835 0.51439232 0.5137621  0.52193911]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.6716524911260044\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.03022762 0.03275051]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.2410747  -0.24091714]]\n",
      "New Weight: [[0.05433509 0.05684223]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.02483467]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.23592453]]\n",
      "New Bias: [[0.04842713]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.51210442 0.52629306 0.52566797 0.53981663]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.6547905528041742\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.05433509 0.05684223]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.23362885 -0.23347258]]\n",
      "New Weight: [[0.07769798 0.08018948]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.04842713]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.22402948]]\n",
      "New Bias: [[0.07083008]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.51770012 0.5376833  0.5370639  0.55693142]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.6391565313200924\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.07769798 0.08018948]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.22650117 -0.22634632]]\n",
      "New Weight: [[0.1003481  0.10282412]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.07083008]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.21265532]]\n",
      "New Bias: [[0.09209561]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.52300764 0.54857623 0.54796299 0.57328529]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.624650790852522\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.1003481  0.10282412]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.21968793 -0.21953462]]\n",
      "New Weight: [[0.12231689 0.12477758]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.09209561]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.20179196]]\n",
      "New Bias: [[0.1122748]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.52803925 0.55898713 0.55838043 0.58888774]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.6111806922889851\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.12231689 0.12477758]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.21318296 -0.21303128]]\n",
      "New Weight: [[0.14363518 0.14608071]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.1122748]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.19142636]]\n",
      "New Bias: [[0.13141744]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.53280716 0.56893275 0.56833289 0.60375441]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5986604783996269\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.14363518 0.14608071]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.20697817 -0.20682821]]\n",
      "New Weight: [[0.164333   0.16676353]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.13141744]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.18154319]]\n",
      "New Bias: [[0.14957176]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.53732338 0.57843087 0.57783808 0.61790567]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5870110520884311\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.164333   0.16676353]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.20106406 -0.20091586]]\n",
      "New Weight: [[0.18443941 0.18685511]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.14957176]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.1721255]]\n",
      "New Bias: [[0.16678431]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.54159969 0.58749985 0.5869143  0.63136539]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5761596789001345\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.18443941 0.18685511]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.19543008 -0.19528369]]\n",
      "New Weight: [[0.20398242 0.20638348]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.16678431]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.16315519]]\n",
      "New Bias: [[0.18309983]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.5456475  0.59615831 0.59558011 0.64415995]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5660396395836904\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.20398242 0.20638348]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.19006498 -0.18992044]]\n",
      "New Weight: [[0.22298891 0.22537553]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.18309983]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.15461353]]\n",
      "New Bias: [[0.19856118]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.54947784 0.60442488 0.60385411 0.65631732]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5565898534391591\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.22298891 0.22537553]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.18495714 -0.18481445]]\n",
      "New Weight: [[0.24148463 0.24385697]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.19856118]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.14648146]]\n",
      "New Bias: [[0.21320933]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.55310133 0.61231799 0.61175468 0.66786642]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5477544886256944\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.24148463 0.24385697]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.18009473 -0.1799539 ]]\n",
      "New Weight: [[0.2594941  0.26185236]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.21320933]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.1387399]]\n",
      "New Bias: [[0.22708332]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.55652812 0.61985567 0.61929983 0.67883646]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5394825716847385\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.2594941  0.26185236]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.17546593 -0.17532697]]\n",
      "New Weight: [[0.27704069 0.27938506]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.22708332]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.13136998]]\n",
      "New Bias: [[0.24022032]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.55976794 0.62705548 0.62650708 0.68925659]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5317276052531015\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.27704069 0.27938506]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.17105908 -0.17092198]]\n",
      "New Weight: [[0.2941466  0.29647726]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.24022032]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.12435323]]\n",
      "New Bias: [[0.25265564]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.56283004 0.63393439 0.63339337 0.69915547]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5244472002718921\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.2941466  0.29647726]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.16686279 -0.16672754]]\n",
      "New Weight: [[0.31083288 0.31315001]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.25265564]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.11767168]]\n",
      "New Bias: [[0.26442281]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.5657232  0.64050872 0.63997501 0.70856105]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5176027268771385\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.31083288 0.31315001]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.16286598 -0.16273256]]\n",
      "New Weight: [[0.32711948 0.32942326]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.26442281]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.111308]]\n",
      "New Bias: [[0.27555361]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.5684558  0.64679411 0.64626763 0.7175004 ]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5111589865116416\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.32711948 0.32942326]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.15905799 -0.15892637]]\n",
      "New Weight: [[0.34302528 0.3453159 ]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.27555361]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.10524552]]\n",
      "New Bias: [[0.28607816]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.57103573 0.65280549 0.65228614 0.72599949]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.5050839065482918\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.34302528 0.3453159 ]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.15542859 -0.15529875]]\n",
      "New Weight: [[0.35856814 0.36084578]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.28607816]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.09946829]]\n",
      "New Bias: [[0.29602499]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.57347051 0.6585571  0.65804477 0.7340832 ]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.4993482577902776\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.35856814 0.36084578]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.15196801 -0.15183992]]\n",
      "New Weight: [[0.37376494 0.37602977]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.29602499]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.09396111]]\n",
      "New Bias: [[0.3054211]]\n",
      "\n",
      "Prediction and Loss:\n",
      "\n",
      "Epoch Number: 20\n",
      "Forecast Value: [[0.57576721 0.66406244 0.66355701 0.74177522]]\n",
      "Actual Value: [[0 1 1 1]]\n",
      "Loss: 0.49392539454844925\n",
      "\n",
      "Parameter Update:\n",
      "Weight:\n",
      "Old Weight: [[0.37376494 0.37602977]]\n",
      "Learning rate: 0.1\n",
      "dWeight: [[-0.14866694 -0.14854059]]\n",
      "New Weight: [[0.38863163 0.39088383]]\n",
      "\n",
      "Bias: \n",
      "Old Bias: [[0.3054211]]\n",
      "Learning Rate: 0.1\n",
      "dBias: [[-0.08870953]]\n",
      "New Bias: [[0.31429205]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i in range(epochs):\n",
    "    \"\"\"\n",
    "    FORWARD PASS\n",
    "    Here we take the input:\n",
    "    Multiply by the weight,\n",
    "    Add the bias\n",
    "    and apply the activation function (in this case we are using Sigmoid)\n",
    "    \"\"\"\n",
    "    Z = np.dot(Weight, X_input) + Bias\n",
    "\n",
    "    prediction = 1 / (1 + np.exp(-Z)) # Sigmoid Activation with a Logit as input 'Z'\n",
    "\n",
    "    \"\"\"\n",
    "    LOSS CALCULATION\n",
    "    \"\"\"\n",
    "    # number of samples \n",
    "    num_samples = Y_output.shape[1]\n",
    "\n",
    "    # loss function (BCE - Binary Cross Entropy)\n",
    "    epsilon = 1e-15 # using a small value here to avoid log(0)\n",
    "    prediction = np.clip(prediction, epsilon, 1 - epsilon) # setting limits on the values to keep us in our 0, 1 range\n",
    "    loss_bce = - (1 / num_samples) * np.sum(Y_output * np.log(prediction)+(1 - Y_output) * np.log(1 - prediction))\n",
    "\n",
    "    \"\"\"\n",
    "    BACKWARD PASS\n",
    "    Here we take the prediction - Actuals which allows us to calculate:\n",
    "    dWeight - indicates the direction and magnitude of change needed for the weights\n",
    "    dBias - indicates the direction and magnitude of the change needed for the biases\n",
    "\n",
    "    Then we will use those values to update the parameters before the start of the next iteration\n",
    "    \"\"\"\n",
    "    dZ = prediction - Y_output # gradient of the loss with respect to the logit\n",
    "    dWeight = (1 / num_samples) * np.dot(dZ, X_input.T) # gradient of the loss with respect to the weight\n",
    "    dBias = (1 / num_samples) * np.sum(dZ, axis = 1, keepdims = True) # gradient of the loss with respect to the bias\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    PRINT STATEMENTS - Updated Values\n",
    "    So we can view all of the changes and how they are impacting the next iterations\n",
    "    \"\"\"\n",
    "    # pred v acts\n",
    "    print(\"Prediction and Loss:\\n\")\n",
    "    print(f\"Epoch Number: {epochs}\")\n",
    "    print(f\"Forecast Value: {prediction}\")\n",
    "    print(f\"Actual Value: {Y_output}\")\n",
    "    print(f\"Loss: {loss_bce}\\n\")\n",
    "\n",
    "    # Weight update\n",
    "    print(\"Parameter Update:\")\n",
    "    print(\"Weight:\")\n",
    "    print(f\"Old Weight: {Weight}\")\n",
    "    # Updating the Weight Terms\n",
    "    Weight = Weight - learning_rate * dWeight\n",
    "\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"dWeight: {dWeight}\")\n",
    "    print(f\"New Weight: {Weight}\\n\")\n",
    "\n",
    "    # Bias Update\n",
    "    print(\"Bias: \")\n",
    "    print(f\"Old Bias: {Bias}\")\n",
    "    # Updating the Bias Terms\n",
    "    Bias = Bias - learning_rate * dBias\n",
    "\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"dBias: {dBias}\")\n",
    "    print(f\"New Bias: {Bias}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the model runs above:\n",
    "We have hopefully obtained the best loss possible without overfitting to the data. (This is out of the scope of this workbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final output (Forecast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction: [[0.57576721 0.66406244 0.66355701 0.74177522]]\n",
      "Actual Values: [[0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Prediction: {prediction}\")\n",
    "print(f\"Actual Values: {Y_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Weight and Bias Terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weight: [[0.38863163 0.39088383]]\n",
      "Final Bias: [[0.31429205]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Weight: {Weight}\")\n",
    "print(f\"Final Bias: {Bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we will take the final weights and bias values and apply them to out X Input and see how it compares to our Actuals (Y Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Prediction: [[0.57793255 0.66933433 0.66883567 0.74909802]]\n",
      "Binary Predictions: [[1 1 1 1]]\n",
      "Actuals: [[0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.dot(Weight, X_input) + Bias\n",
    "\n",
    "prediction = 1 / (1 + np.exp(-Z)) # act. func. \n",
    "\n",
    "# convert final probabilities to binary lables for eval\n",
    "binary_prediction = (prediction > .5).astype(int)\n",
    "\n",
    "print(f\"Raw Prediction: {prediction}\")\n",
    "print(f\"Binary Predictions: {binary_prediction}\")\n",
    "print(f\"Actuals: {Y_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
